{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bcbe91-87a6-4666-a0a6-019f50afa2f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pinns/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class AdvectionDiffusionPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Neural Network for solving the 2D advection-diffusion equation:\n",
    "    ∂C/∂t = D(∂²C/∂x² + ∂²C/∂y²) - u∂C/∂x - v∂C/∂y + S(x,y,t)\n",
    "    \"\"\"\n",
    "    def __init__(self, D: float, hidden_layers: int = 10, neurons_per_layer: int = 64):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        \n",
    "        # Network architecture\n",
    "        layers = []\n",
    "        # Input layer (x, y, t) -> first hidden layer\n",
    "        layers.append(nn.Linear(3, neurons_per_layer))\n",
    "        layers.append(nn.Tanh())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(nn.Tanh())\n",
    "            \n",
    "        # Output layer (concentration C)\n",
    "        layers.append(nn.Linear(neurons_per_layer, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        inputs = torch.stack([x, y, t], dim=-1)\n",
    "        return self.network(inputs)\n",
    "    \n",
    "    def compute_derivatives(self, x: torch.Tensor, y: torch.Tensor, t: torch.Tensor,\n",
    "                          u: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Compute the derivatives needed for the PDE residual.\"\"\"\n",
    "        # Create variables requiring gradient\n",
    "        x_var = x.clone().detach().requires_grad_(True)\n",
    "        y_var = y.clone().detach().requires_grad_(True)\n",
    "        t_var = t.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        C = self.forward(x_var, y_var, t_var)\n",
    "        \n",
    "        # First derivatives\n",
    "        dC_dt = torch.autograd.grad(C.sum(), t_var, create_graph=True)[0]\n",
    "        dC_dx = torch.autograd.grad(C.sum(), x_var, create_graph=True)[0]\n",
    "        dC_dy = torch.autograd.grad(C.sum(), y_var, create_graph=True)[0]\n",
    "        \n",
    "        # Second derivatives\n",
    "        d2C_dx2 = torch.autograd.grad(dC_dx.sum(), x_var, create_graph=True)[0]\n",
    "        d2C_dy2 = torch.autograd.grad(dC_dy.sum(), y_var, create_graph=True)[0]\n",
    "        \n",
    "        return dC_dt, dC_dx, dC_dy, d2C_dx2, d2C_dy2\n",
    "    \n",
    "    def pde_residual(self, x: torch.Tensor, y: torch.Tensor, t: torch.Tensor,\n",
    "                    u: torch.Tensor, v: torch.Tensor, S: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the PDE residual:\n",
    "        R = ∂C/∂t - D(∂²C/∂x² + ∂²C/∂y²) + u∂C/∂x + v∂C/∂y - S\n",
    "        \"\"\"\n",
    "        dC_dt, dC_dx, dC_dy, d2C_dx2, d2C_dy2 = self.compute_derivatives(x, y, t, u, v)\n",
    "        \n",
    "        # Compute diffusion term\n",
    "        diffusion_term = self.D * (d2C_dx2 + d2C_dy2)\n",
    "        \n",
    "        # Compute advection term\n",
    "        advection_term = u * dC_dx + v * dC_dy\n",
    "        \n",
    "        # Full residual\n",
    "        residual = dC_dt - diffusion_term + advection_term - S\n",
    "        \n",
    "        return residual, dC_dt, dC_dx, dC_dy, d2C_dx2, d2C_dy2\n",
    "\n",
    "def prepare_temporal_split_data(ds: xr.Dataset, train_days: int = 70, \n",
    "                           spatial_subsample: int = 4,\n",
    "                           temporal_subsample: int = 4) -> Dict[str, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Prepare data with temporal train-test split.\n",
    "    First 70 days for training, last 30 days for testing.\n",
    "    \"\"\"\n",
    "    # Calculate split point in timesteps (24 timesteps per day)\n",
    "    hours_per_day = 24\n",
    "    train_timesteps = train_days * hours_per_day\n",
    "    \n",
    "    # Subsample coordinates first\n",
    "    x_coords = ds.xC.values[::spatial_subsample]\n",
    "    y_coords = ds.yC.values[::spatial_subsample]\n",
    "    \n",
    "    # Handle temporal subsampling and splitting\n",
    "    all_timesteps = np.arange(len(ds.time))\n",
    "    subsampled_timesteps = all_timesteps[::temporal_subsample]\n",
    "    t_coords = subsampled_timesteps * 3600  # Convert to seconds\n",
    "    \n",
    "    # Create temporal mask based on subsampled timesteps\n",
    "    time_train_mask = subsampled_timesteps <= train_timesteps\n",
    "    time_test_mask = ~time_train_mask\n",
    "    \n",
    "    # Print information about subsampling\n",
    "    print(\"\\nSubsampled grid sizes:\")\n",
    "    print(f\"X points: {len(x_coords)} (original: {len(ds.xC.values)})\")\n",
    "    print(f\"Y points: {len(y_coords)} (original: {len(ds.yC.values)})\")\n",
    "    print(f\"Time points: {len(t_coords)} (original: {len(ds.time)})\")\n",
    "    \n",
    "    # Get the data arrays\n",
    "    u_array = ds.u.values[::temporal_subsample, :, ::spatial_subsample, ::spatial_subsample]\n",
    "    v_array = ds.v.values[::temporal_subsample, :, ::spatial_subsample, ::spatial_subsample]\n",
    "    c_array = ds.c.values[::temporal_subsample, :, ::spatial_subsample, ::spatial_subsample]\n",
    "    \n",
    "    # Create coordinate grids for training data\n",
    "    train_times = t_coords[time_train_mask]\n",
    "    test_times = t_coords[time_test_mask]\n",
    "    \n",
    "    # Create meshgrids\n",
    "    X_train, Y_train, T_train = np.meshgrid(x_coords, y_coords, train_times, indexing='ij')\n",
    "    X_test, Y_test, T_test = np.meshgrid(x_coords, y_coords, test_times, indexing='ij')\n",
    "    \n",
    "    # Print debug information\n",
    "    print(\"\\nData shapes:\")\n",
    "    print(f\"Original u shape: {ds.u.values.shape}\")\n",
    "    print(f\"Subsampled u shape: {u_array.shape}\")\n",
    "    print(f\"Time mask shape: {time_train_mask.shape}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"Training data shape: {u_array[time_train_mask].shape}\")\n",
    "    print(f\"Number of training timesteps: {sum(time_train_mask)}\")\n",
    "    print(f\"Number of test timesteps: {sum(time_test_mask)}\")\n",
    "    \n",
    "    # Reshape velocity data for training\n",
    "    u_train = u_array[time_train_mask].reshape(-1)\n",
    "    v_train = v_array[time_train_mask].reshape(-1)\n",
    "    c_train = c_array[time_train_mask].reshape(-1)\n",
    "    \n",
    "    # Reshape velocity data for testing\n",
    "    u_test = u_array[time_test_mask].reshape(-1)\n",
    "    v_test = v_array[time_test_mask].reshape(-1)\n",
    "    c_test = c_array[time_test_mask].reshape(-1)\n",
    "    \n",
    "    # Create training tensors\n",
    "    train_data = {\n",
    "        'x': torch.tensor(X_train.flatten(), dtype=torch.float32),\n",
    "        'y': torch.tensor(Y_train.flatten(), dtype=torch.float32),\n",
    "        't': torch.tensor(T_train.flatten(), dtype=torch.float32),\n",
    "        'u': torch.tensor(u_train, dtype=torch.float32),\n",
    "        'v': torch.tensor(v_train, dtype=torch.float32),\n",
    "        'c': torch.tensor(c_train, dtype=torch.float32),\n",
    "    }\n",
    "    \n",
    "    # Create test tensors\n",
    "    test_data = {\n",
    "        'x': torch.tensor(X_test.flatten(), dtype=torch.float32),\n",
    "        'y': torch.tensor(Y_test.flatten(), dtype=torch.float32),\n",
    "        't': torch.tensor(T_test.flatten(), dtype=torch.float32),\n",
    "        'u': torch.tensor(u_test, dtype=torch.float32),\n",
    "        'v': torch.tensor(v_test, dtype=torch.float32),\n",
    "        'c': torch.tensor(c_test, dtype=torch.float32),\n",
    "    }\n",
    "    \n",
    "    # Handle source term if present\n",
    "    if 'S' in ds:\n",
    "        s_array = ds.S.values[::temporal_subsample, :, ::spatial_subsample, ::spatial_subsample]\n",
    "        train_data['S'] = torch.tensor(s_array[time_train_mask].reshape(-1), dtype=torch.float32)\n",
    "        test_data['S'] = torch.tensor(s_array[time_test_mask].reshape(-1), dtype=torch.float32)\n",
    "    else:\n",
    "        train_data['S'] = torch.zeros_like(train_data['x'])\n",
    "        test_data['S'] = torch.zeros_like(test_data['x'])\n",
    "    \n",
    "    # Print final dataset sizes\n",
    "    print(\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Training samples: {len(train_data['x'])}\")\n",
    "    print(f\"Test samples: {len(test_data['x'])}\")\n",
    "    \n",
    "    return {'train': train_data, 'test': test_data}\n",
    "\n",
    "def train_pinn(model: AdvectionDiffusionPINN, \n",
    "               data: Dict[str, torch.Tensor],\n",
    "               num_epochs: int = 100, \n",
    "               learning_rate: float = 0.01,\n",
    "               batch_size: int = 1000,\n",
    "               save_dir: str = \"models\") -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Train the PINN model using mini-batches.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    n_samples = len(data['x'])\n",
    "    history = {'loss': [], 'pde_loss': [], 'data_loss': []}\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting training with {n_samples} samples...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Weighted loss\n",
    "    pde_weight = 1.0\n",
    "    data_weight = \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Random permutation for batching\n",
    "        perm = torch.randperm(n_samples)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_pde_loss = 0.0\n",
    "        running_data_loss = 0.0\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get batch indices\n",
    "            idx = perm[i:i + batch_size]\n",
    "            \n",
    "            # Get batch data\n",
    "            x_batch = data['x'][idx]\n",
    "            y_batch = data['y'][idx]\n",
    "            t_batch = data['t'][idx]\n",
    "            u_batch = data['u'][idx]\n",
    "            v_batch = data['v'][idx]\n",
    "            c_batch = data['c'][idx]\n",
    "            S_batch = data['S'][idx]\n",
    "            \n",
    "            # Compute PDE residual\n",
    "            residual, dC_dt, dC_dx, dC_dy, d2C_dx2, d2C_dy2 = model.pde_residual(\n",
    "                x_batch, y_batch, t_batch, u_batch, v_batch, S_batch)\n",
    "\n",
    "            pde_loss = torch.mean(residual**2)\n",
    "            \n",
    "            # Compute predicted concentration\n",
    "            c_pred = model(x_batch, y_batch, t_batch)\n",
    "            data_loss = torch.mean((c_pred - c_batch)**2)\n",
    "\n",
    "            # Combined loss without weights\n",
    "            #loss = pde_loss + data_loss\n",
    "\n",
    "            # Combined loss with weights\n",
    "            loss = pde_weight * pde_loss + data_weight * data_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_pde_loss += pde_loss.item()\n",
    "            running_data_loss += data_loss.item()\n",
    "\n",
    "            #if i % 1000 == 0:\n",
    "             #   print(f\"Batch i={i}, PDE={pde_loss.item()}, Data={data_loss.item()}\") \n",
    "            \n",
    "        # Record losses\n",
    "        avg_loss = running_loss / (n_samples / batch_size)\n",
    "        avg_pde_loss = running_pde_loss / (n_samples / batch_size)\n",
    "        avg_data_loss = running_data_loss / (n_samples / batch_size)\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['pde_loss'].append(avg_pde_loss)\n",
    "        history['data_loss'].append(avg_data_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'-----Epoch {epoch}, Loss: {avg_loss:.6f}, PDE Loss: {avg_pde_loss:.6f}, '\n",
    "                  f'Data Loss: {avg_data_loss:.6f}, Time: {elapsed:.2f}s')\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, save_dir / f'pinn_checkpoint_epoch_{epoch}.pt')\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model: nn.Module, data: Dict[str, torch.Tensor], \n",
    "                  batch_size: int = 1000) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model performance on given dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_pde_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # First collect predictions without gradients\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data['x']), batch_size):\n",
    "            # Get batch\n",
    "            x_batch = data['x'][i:i + batch_size]\n",
    "            y_batch = data['y'][i:i + batch_size]\n",
    "            t_batch = data['t'][i:i + batch_size]\n",
    "            \n",
    "            # Get predictions\n",
    "            pred = model(x_batch, y_batch, t_batch)\n",
    "            predictions.append(pred)\n",
    "            actuals.append(data['c'][i:i + batch_size])\n",
    "    \n",
    "    # Then compute PDE residuals with gradients enabled\n",
    "    for i in range(0, len(data['x']), batch_size):\n",
    "        # Get batch\n",
    "        x_batch = data['x'][i:i + batch_size]\n",
    "        y_batch = data['y'][i:i + batch_size]\n",
    "        t_batch = data['t'][i:i + batch_size]\n",
    "        u_batch = data['u'][i:i + batch_size]\n",
    "        v_batch = data['v'][i:i + batch_size]\n",
    "        S_batch = data['S'][i:i + batch_size]\n",
    "        \n",
    "        # Compute PDE residual with gradients enabled\n",
    "        residual, *_ = model.pde_residual(x_batch, y_batch, t_batch, \n",
    "                                    u_batch, v_batch, S_batch)\n",
    "        total_pde_loss += torch.mean(residual**2).item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Concatenate predictions and actual values\n",
    "    predictions = torch.cat(predictions, dim=0).numpy()\n",
    "    actuals = torch.cat(actuals, dim=0).numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    avg_pde_loss = total_pde_loss / n_batches\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'pde_loss': avg_pde_loss\n",
    "    }\n",
    "\n",
    "def plot_results(model: nn.Module, data: Dict[str, torch.Tensor], \n",
    "                ds: xr.Dataset, timestep: int, save_dir: str = \"figures\") -> None:\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted concentration at a specific timestep.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    x_coords = ds.xC.values\n",
    "    y_coords = ds.yC.values\n",
    "    \n",
    "    # Create meshgrid for single timestep\n",
    "    X, Y = np.meshgrid(x_coords, y_coords)\n",
    "    T = np.full_like(X, timestep * 3600)  # Convert to seconds\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        x_tensor = torch.tensor(X.flatten(), dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(Y.flatten(), dtype=torch.float32)\n",
    "        t_tensor = torch.tensor(T.flatten(), dtype=torch.float32)\n",
    "        predictions = model(x_tensor, y_tensor, t_tensor)\n",
    "    \n",
    "    # Reshape predictions\n",
    "    pred_grid = predictions.numpy().reshape(X.shape)\n",
    "    actual_grid = np.squeeze(ds.c.isel(time=timestep).values)\n",
    "\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot actual concentration\n",
    "    im1 = ax1.pcolormesh(X, Y, actual_grid, shading='auto')\n",
    "    ax1.set_title('Actual Concentration')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Plot predicted concentration\n",
    "    im2 = ax2.pcolormesh(X, Y, pred_grid, shading='auto')\n",
    "    ax2.set_title('Predicted Concentration')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(save_dir / f'comparison_timestep_{timestep}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_history(history: Dict[str, list], save_dir: str = \"figures\") -> None:\n",
    "    \"\"\"\n",
    "    Plot training history including total loss, PDE loss, and data loss.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history['loss'], label='Total Loss')\n",
    "    plt.plot(history['pde_loss'], label='PDE Loss')\n",
    "    plt.plot(history['data_loss'], label='Data Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_dir / 'training_history.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374fddb0-e6d8-4718-a236-e58a1ce0d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Preparing temporal train-test split...\n",
      "\n",
      "Subsampled grid sizes:\n",
      "X points: 25 (original: 100)\n",
      "Y points: 25 (original: 100)\n",
      "Time points: 601 (original: 2401)\n",
      "\n",
      "Data shapes:\n",
      "Original u shape: (2401, 1, 100, 100)\n",
      "Subsampled u shape: (601, 1, 25, 25)\n",
      "Time mask shape: (601,)\n",
      "X_train shape: (25, 25, 421)\n",
      "Training data shape: (421, 1, 25, 25)\n",
      "Number of training timesteps: 421\n",
      "Number of test timesteps: 180\n",
      "\n",
      "Final dataset sizes:\n",
      "Training samples: 263125\n",
      "Test samples: 112500\n",
      "\n",
      "Dataset information:\n",
      "Training samples: 263125\n",
      "Test samples: 112500\n",
      "\n",
      "Initializing PINN model...\n",
      "\n",
      "Starting training...\n",
      "Starting training with 263125 samples...\n",
      "-----Epoch 0, Loss: 1229.073644, PDE Loss: 1229.073634, Data Loss: 0.000014, Time: 3.38s\n",
      "-----Epoch 10, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000001, Time: 43.28s\n",
      "-----Epoch 20, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 80.21s\n",
      "-----Epoch 30, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000001, Time: 118.14s\n",
      "-----Epoch 40, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 156.96s\n",
      "-----Epoch 50, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 193.84s\n",
      "-----Epoch 60, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 232.78s\n",
      "-----Epoch 70, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 274.69s\n",
      "-----Epoch 80, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 311.47s\n",
      "-----Epoch 90, Loss: 1229.073634, PDE Loss: 1229.073634, Data Loss: 0.000000, Time: 351.70s\n",
      "\n",
      "Plotting training history...\n",
      "\n",
      "Evaluating model on test set...\n",
      "\n",
      "Test Set Metrics:\n",
      "MSE: 0.000000\n",
      "RMSE: 0.000302\n",
      "R²: -0.232653\n",
      "PDE Loss: 1225.000000\n",
      "\n",
      "Generating visualization plots...\n",
      "Plotting timestep 1800...\n",
      "Plotting timestep 1900...\n",
      "Plotting timestep 2000...\n",
      "Plotting timestep 2100...\n",
      "\n",
      "Training and evaluation complete!\n",
      "Models saved in: models\n",
      "Figures saved in: figures\n"
     ]
    }
   ],
   "source": [
    "# Main execution code\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DAYS = 70\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 1000\n",
    "DIFFUSION_COEFF = 1e-4\n",
    "HIDDEN_LAYERS = 5\n",
    "NEURONS_PER_LAYER = 32\n",
    "\n",
    "# Create directories for saving results\n",
    "MODEL_DIR = Path(\"models\")\n",
    "FIGURE_DIR = Path(\"figures\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading dataset...\")\n",
    "ds = xr.open_dataset(\"output-tracer-release_2025-02-16.nc\")\n",
    "\n",
    "# Prepare train-test split\n",
    "print(\"Preparing temporal train-test split...\")\n",
    "data = prepare_temporal_split_data(ds, train_days=TRAIN_DAYS)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"\\nDataset information:\")\n",
    "print(f\"Training samples: {len(data['train']['x'])}\")\n",
    "print(f\"Test samples: {len(data['test']['x'])}\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing PINN model...\")\n",
    "model = AdvectionDiffusionPINN(\n",
    "    D=DIFFUSION_COEFF,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    neurons_per_layer=NEURONS_PER_LAYER\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "history = train_pinn(\n",
    "    model=model,\n",
    "    data=data['train'],\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    save_dir=str(MODEL_DIR)\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "plot_training_history(history, save_dir=str(FIGURE_DIR))\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "test_metrics = evaluate_model(model, data['test'])\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"MSE: {test_metrics['mse']:.6f}\")\n",
    "print(f\"RMSE: {test_metrics['rmse']:.6f}\")\n",
    "print(f\"R²: {test_metrics['r2']:.6f}\")\n",
    "print(f\"PDE Loss: {test_metrics['pde_loss']:.6f}\")\n",
    "\n",
    "# Plot results for multiple timesteps in test period\n",
    "print(\"\\nGenerating visualization plots...\")\n",
    "test_timesteps = [1800, 1900, 2000, 2100]  # Example timesteps in test period\n",
    "for timestep in test_timesteps:\n",
    "    print(f\"Plotting timestep {timestep}...\")\n",
    "    plot_results(model, data['test'], ds, timestep, save_dir=str(FIGURE_DIR))\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")\n",
    "print(f\"Models saved in: {MODEL_DIR}\")\n",
    "print(f\"Figures saved in: {FIGURE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1435368-9ec7-4224-a3d2-e425c306d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.u.values.shape  # Likely (2401, 1, 100, 100) or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a47f0-50ba-4335-8bc6-69c74ae208fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
